{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 광주 인공지능 사관학교\n",
    "- - -\n",
    "- 작성자 : 2반 한지호\n",
    "- 작성일 : 20.07.21 화\n",
    "- 4교시 자연어 처리 시간에 학습한 tf-idf 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    '안녕하세요 감사해요 잘있어요 다시 만나요',\n",
    "    '안녕하세요 오늘도 좋은 하루 보내세요',\n",
    "    '안녕하세요 안녕하세요 안녕하세요 오늘도 날씨가 좋네요',\n",
    "]\n",
    "\n",
    "# 문서 전체의 단어들을 토큰화\n",
    "token_list = Okt().morphs(' '.join(doc_list))\n",
    "# 중복 단어를 제거하고자\n",
    "token_list = list(set(token_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, document):\n",
    "    # document 에서 term 의 등장 횟수를 count\n",
    "    return document.count(term)\n",
    "\n",
    "def idf(term):\n",
    "    # doc_list 에서 term 이 등장한 문서 수를 count\n",
    "    df = 0 # 함수가 돌 때마다 초기화\n",
    "    for doc in doc_list:\n",
    "        # 각 문서마다 해당 단어가 있는지 확인\n",
    "        if term in doc:\n",
    "            df = df + 1\n",
    "    return log(len(doc_list)/(df+1)) + 1 # 단어 빈도가 문서 수보다 더 많아 음수가 나오는 것을 방지하기 위해 +1\n",
    "        \n",
    "def tfidf(term, document):\n",
    "    # tf * idf\n",
    "    return tf(term, document) * idf(term) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   오늘  다시  감사해요  도  좋네요  잘있어요  만나요  가  안녕하세요  하루  날씨  좋은  보내세요\n",
      "0   0   1     1  0    0     1    1  0      1   0   0   0     0\n",
      "1   1   0     0  1    0     0    0  0      1   1   0   1     1\n",
      "2   1   0     0  1    1     0    0  1      3   0   1   0     0\n"
     ]
    }
   ],
   "source": [
    "dtm = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    # document term matrix (문서별 단어 등장 횟수) 를 구현해보자, 3 (문서) x 13 (단어) 의 행렬을 리스트로 구성하면 된다.\n",
    "    dtm.append([]) # 행으로 리스트를 만들어 추가\n",
    "    for token in token_list:\n",
    "        dtm[-1].append(tf(token, doc))\n",
    "        \n",
    "# print(token_list) \n",
    "# print(dtm) \n",
    "    \n",
    "dtm_pd = pd.DataFrame(dtm, columns=token_list)\n",
    "print(dtm_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            idf\n",
      "오늘     1.000000\n",
      "다시     1.405465\n",
      "감사해요   1.405465\n",
      "도      1.000000\n",
      "좋네요    1.405465\n",
      "잘있어요   1.405465\n",
      "만나요    1.405465\n",
      "가      1.405465\n",
      "안녕하세요  0.712318\n",
      "하루     1.405465\n",
      "날씨     1.405465\n",
      "좋은     1.405465\n",
      "보내세요   1.405465\n"
     ]
    }
   ],
   "source": [
    "idf_list = []\n",
    "\n",
    "for token in token_list:\n",
    "    # 단어별로 idf의 값을 구한 리스트를 만들어보자\n",
    "    # 위 idf 함수를 이용하면 된다\n",
    "    idf_list.append(idf(token)) \n",
    "    \n",
    "\n",
    "idf_pd = pd.DataFrame(idf_list, columns=['idf'], index=token_list)\n",
    "print(idf_pd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    오늘        다시      감사해요    도       좋네요      잘있어요       만나요         가  \\\n",
      "0  0.0  1.405465  1.405465  0.0  0.000000  1.405465  1.405465  0.000000   \n",
      "1  1.0  0.000000  0.000000  1.0  0.000000  0.000000  0.000000  0.000000   \n",
      "2  1.0  0.000000  0.000000  1.0  1.405465  0.000000  0.000000  1.405465   \n",
      "\n",
      "      안녕하세요        하루        날씨        좋은      보내세요  \n",
      "0  0.712318  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.712318  1.405465  0.000000  1.405465  1.405465  \n",
      "2  2.136954  0.000000  1.405465  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "tfidf_list = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    # tfidf 를 구현해보자, 3 (문서) x 13 (단어) 의 행렬을 리스트로 구성하면 된다\n",
    "    # 내부의 요소는 모두 tfidf 값으로 구성\n",
    "    tfidf_list.append([]) \n",
    "    for token in token_list:\n",
    "        tfidf_list[-1].append(tfidf(token, doc)) \n",
    "\n",
    "tfidf_pd = pd.DataFrame(tfidf_list, columns=token_list)\n",
    "print(tfidf_pd) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
