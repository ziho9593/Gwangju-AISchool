{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy\n",
    "\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "doc_list = [\n",
    "    '안녕하세요 감사해요 잘있어요 다시 만나요',\n",
    "    '안녕하세요 오늘도 좋은 하루 보내세요',\n",
    "    '안녕하세요 안녕하세요 안녕하세요 오늘도 날씨가 좋네요',\n",
    "]\n",
    "\n",
    "# 문서 전체의 단어들을 토큰화\n",
    "token_list = Okt().morphs(' '.join(doc_list))\n",
    "# 중복 단어를 제거하고자\n",
    "token_list = list(set(token_list))\n",
    "\n",
    "def tf(term, document):\n",
    "    return document.count(term)\n",
    "\n",
    "def idf(term):\n",
    "    # 함수가 돌 때마다 초기화\n",
    "    df = 0\n",
    "    for doc in doc_list:\n",
    "        # 각 문서마다 해당 단어가 있는지 확인\n",
    "        if term in doc:\n",
    "            df = df + 1\n",
    "    return log(len(doc_list)/(df+1))+1\n",
    "\n",
    "def tfidf(term, document):\n",
    "    return tf(term, document)*idf(term)\n",
    "\n",
    "dtm = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    # 방금 추가한 문서의 리스트\n",
    "    # 문서로서의 행\n",
    "    dtm.append([])\n",
    "    for token in token_list:\n",
    "        # -1은 마지막 인덱스\n",
    "        dtm[-1].append(tf(token, doc))\n",
    "\n",
    "# [[]]\n",
    "# [[단어1의 tf, 단어2의 tf ....]] -> 문서1 역할을 할 행\n",
    "# [[단어1의 tf ~~~~...], [단어1의 tf...], []]\n",
    "\n",
    "# print(token_list)\n",
    "# print(dtm)\n",
    "# print('\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "dtm_pd = pd.DataFrame(dtm, columns=token_list)\n",
    "print(dtm_pd)\n",
    "\n",
    "idf_list = []\n",
    "\n",
    "for token in token_list:\n",
    "    idf_list.append(idf(token))\n",
    "\n",
    "idf_pd = pd.DataFrame(idf_list, columns=['idf'], index=token_list)\n",
    "print(idf_pd)\n",
    "\n",
    "tfidf_list = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    tfidf_list.append([])\n",
    "    for token in token_list:\n",
    "        tfidf_list[-1].append(tfidf(token, doc))\n",
    "\n",
    "tfidf_pd = pd.DataFrame(tfidf_list, columns=token_list)\n",
    "print(tfidf_pd)"
   ]
  }
 ]
}