{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2UN44T17J3Q",
        "colab_type": "text"
      },
      "source": [
        "## Glove, Word2Vec 링크\n",
        "\n",
        "Glove 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl4t5muA7UcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import csv\n",
        "import re\n",
        "\n",
        "def open_csv():\n",
        "    # csv파일을 연다!\n",
        "    f = open('IMDB Dataset.csv', 'r', encoding='utf-8')\n",
        "    csvreader = csv.reader(f)\n",
        "    \n",
        "    doc_list = []\n",
        "\n",
        "    next(csvreader)\n",
        "    for f in csvreader:\n",
        "        line = re.compile(\"[^\\w]\").sub(' ', f[0].lower())\n",
        "        doc_list.append(line.split())\n",
        "\n",
        "    return doc_list\n",
        "\n",
        "doc_list = open_csv()\n",
        "print(doc_list[:3])\n",
        "\n",
        "model = Word2Vec(sentences=doc_list, size=100, window=3, min_count=3, workers=4, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_jz17C7LXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#저장\n",
        "model.wv.save_word2vec_format('imdb_w2v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4bl1N1a7T16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m gensim.scripts.word2vec2tensor --input imdb_w2v --output imdb_w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujHrwNhu7Zys",
        "colab_type": "text"
      },
      "source": [
        "## 이미 만들어진 모델을 가지고 와서 쓰는 법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDpQx7kwN-1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "token = Tokenizer()\n",
        "text = \"Keras tokenizer is good\"\n",
        "#원래는 유니크한 단어를 세야한다\n",
        "word_count = len(text)\n",
        "token.fit_on_texts([text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1oY0UIOM_2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "f = open('./drive/My Drive/glove.6B.50d.txt', encoding=\"utf8\")\n",
        "\n",
        "word_vectors = {}\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word_vectors[word_vector[0]] = word_vector[1:]\n",
        "\n",
        "print(word_vectors['apple'])\n",
        "\n",
        "# 단어갯수 x 임베딩 벡터의 차원의 빈 행렬을 생성\n",
        "matrix = np.zeros((4, 50))\n",
        "\n",
        "# 예시용, 실제로는 word가 있을 경우에만 추가를 해줘야함! (예외처리 필요)\n",
        "for word, i in token.word_index.items():\n",
        "    matrix[i] = np.array(word_vectors[word])\n",
        "\n",
        "# 레이어를 쌓아나갈 모델을 생성\n",
        "model = Sequential()\n",
        "# 임베딩을 위한 레이어를 생성\n",
        "model.add(Embedding(4, 50, weights=[matrix]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_BXSRYKPSfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Word2Vec을 사용할때에 (이미 딕셔너리 형태처럼 정리가 되어있음)\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
        "\n",
        "# 각 단어들에 대응하는 벡터를 저장할 배열 생성\n",
        "matrix = np.zeros((word_count, 300))\n",
        "\n",
        "# 예시용, 실제로는 word가 있을 경우에만 추가를 해줘야함! (예외처리 필요)\n",
        "for word, i in token.word_index.items():\n",
        "    matrix[i] = np.array(word2vec_model['word'])\n",
        "\n",
        "# 예시일뿐 실제로 사용가능한 모델은 아님\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_count, 100, weights=[matrix], input_length=4))\n",
        "# 이후 dense layer 로 갈때 flatten layer가 필요 (차원맞추기위함)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}