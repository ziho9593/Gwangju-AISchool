{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cizrTAeoOKDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kN3kGx4x82E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbaVwtRym-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# encoding은 보통 utf-8, cp949 로 하면되지만 이번 파일은 latin1\n",
        "spam_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ai_school/data/spam.csv', encoding='latin1')\n",
        "\n",
        "# 3, 4, 5열 삭제 후 컬럼명 변경\n",
        "spam_data = spam_data.dropna(axis=1)\n",
        "spam_data.columns = [\"label\", \"mail\"]\n",
        "\n",
        "# ham, spam 숫자로 변경\n",
        "spam_data['label'] = spam_data['label'].replace('spam', 1)\n",
        "spam_data['label'] = spam_data['label'].replace('ham', 0)\n",
        "\n",
        "# 단어 아니면 삭제\n",
        "spam_data['mail'] = spam_data['mail'].str.replace(\"[^\\w]\", \" \") # str.replace를 해야 정규표현식 사용 가능\n",
        "\n",
        "# 혹시나 공백이 있으면\n",
        "spam_data['mail'] = spam_data['mail'].replace('', np.nan)\n",
        "spam_data['label'] = spam_data['label'].replace('', np.nan)\n",
        "\n",
        "# 결측치 있으면 모두 제거\n",
        "spam_data = spam_data.dropna(how='any')\n",
        "\n",
        "print(\"# preprocessing done\")\n",
        "\n",
        "\n",
        "# test/train 스플릿하고\n",
        "mail_train, mail_test, y_train, y_test = train_test_split(spam_data['mail'], spam_data['label'], test_size=0.2, shuffle=False)\n",
        "\n",
        "print('# split done')\n",
        "\n",
        "\n",
        "stopwords = ['a', 'an']\n",
        "\n",
        "# 토큰화 진행\n",
        "X_train = []\n",
        "for stc in mail_train:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_train.append(token)\n",
        "\n",
        "X_test = []\n",
        "for stc in mail_test:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_test.append(token)\n",
        "\n",
        "print('# tokenization done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQjcAg-O00y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# X_train 단어들을 토대로 정수 인덱스 설정\n",
        "# 빈도수가 높은 것부터 4000개만 정수 인덱스로 변환하겠다!\n",
        "tokenizer = Tokenizer(4000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 위에서 설정된 정수 인덱스를 토대로 변환\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print('# int_encoding done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAE4DkHh0-w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(tokenizer.word_index)) # 전체 단어 갯수\n",
        "\n",
        "low_count = 0\n",
        "for word, word_count in tokenizer.word_counts.items():\n",
        "    if word_count == 1:\n",
        "        low_count += 1\n",
        "print(low_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZjvF6OH1uY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 0\n",
        "for data in X_train:\n",
        "    if max_length < len(data):\n",
        "        max_length = len(data)\n",
        "print(max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EwHTags1_zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 패딩 작업 진행\n",
        "# 대부분 40~50의 길이를 갖기 때문에, 원래의 max_length가 아닌 50으로 지정하는 것이 효율적.\n",
        "max_len = 50\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9dqLAxA2HNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(4000, 32)) # 최대 128이지만, 데이터셋이 단순한 편이라 32로 지정\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD70Lzpq2rn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsQubEr43xO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = input()\n",
        "# 토큰화\n",
        "token_stc = sentence.split()\n",
        "# 정수 인코딩\n",
        "encode_stc = tokenizer.texts_to_sequences([token_stc])\n",
        "# 패딩\n",
        "pad_stc = pad_sequences(encode_stc, maxlen = 50)\n",
        "\n",
        "score = model.predict(pad_stc)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}