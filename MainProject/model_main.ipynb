{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOc2BG0jlJVcvrtZUd57G/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/master/model_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, optim\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7cVIQlwKPi"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRK4o0qwPbf"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDFmoxfEwNjV"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYrXsg3wYLz"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool):\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypewFuYwMEq"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        self.device = device\n",
        "\n",
        "        if self.extractor == 'vgg':\n",
        "            input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "    conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "    if self.training:\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "    return output, hidden\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        self.wq = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, d_model, bias=True) # ??\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :        \n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention) # TODO : check\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE"
      },
      "source": [
        "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "# out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "# display(out.shape, attn.shape)\n",
        "# display(y)\n",
        "# out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderStep(nn.Module) :\n",
        "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
        "        super(DecoderStep, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var).to(self.device)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if self.training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "        # self.num_layers = num_layers\n",
        "\n",
        "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs) :\n",
        "        assert enc_outputs is not None\n",
        "        \n",
        "        hidden = None\n",
        "        # result, decode_dict = list(), dict()\n",
        "        result = list()\n",
        "\n",
        "        # if not self.training:\n",
        "        #     decode_dict[Speller.KEY_SEQUENCE_SYMBOL] = list()\n",
        "\n",
        "        batch_size = enc_outputs.size(0)\n",
        "\n",
        "        # validate\n",
        "        if inputs is None :\n",
        "            inputs = LongTensor([1] * batch_size).view(batch_size, 1) # [sos_id] * batch_size\n",
        "            max_length = self.max_length\n",
        "        else :\n",
        "            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "\n",
        "        # lengths = np.array([max_length] * batch_size)\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "        for di in range(max_length) :\n",
        "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "            # TODO : check\n",
        "            # validate..?\n",
        "        #     if not self.training :\n",
        "        #         decode_dict[Speller.KEY_SEQUENCE_SYMBOL].append(input_var)\n",
        "        #         eos_batches = input_var.data.eq(2) # eq(eos_id) # input_var와 eos_id가 일치하는지 아닌지를 알 수 있음.\n",
        "\n",
        "        #         if eos_batches.dim() > 0 :\n",
        "        #             eos_batches = eos_batches.cpu().view(-1).numpy()\n",
        "        #             update_idx = ((lengths > di) & eos_batches) != 0\n",
        "        #             lengths[update_idx] = len(decode_dict[Speller.KEY_SEQUENCE_SYMBOL])\n",
        "\n",
        "        # del decode_dict\n",
        "\n",
        "        return result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VWgRi5H-Vp"
      },
      "source": [
        "class LAS(nn.Module) :\n",
        "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
        "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
        "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, input_lengths, targets=None):\n",
        "        output, hidden = self.encoder(inputs, input_lengths)\n",
        "        # print(\"😎 encoding done -> output size : \", output.size())\n",
        "\n",
        "        result = self.decoder(targets, output)\n",
        "        # print(\"🧐 decoder done\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def flatten_parameters(self) :\n",
        "        self.encoder.rnn.flatten_parameters()\n",
        "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbf3XKq4D4bO"
      },
      "source": [
        "## data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8bi0wDfB4yM"
      },
      "source": [
        "# # this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "# !ln -s \"/content/drive/My Drive/\" \"/myDrive\"\n",
        "# !ls \"/myDrive\"\n",
        "# %cd \"/myDrive/SODA\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYHyk6D1LN9"
      },
      "source": [
        "import csv\n",
        "\n",
        "# (train & valid) dataset path가 들어있는 csv 파일 읽어오기\n",
        "# parameter로 train path csv 경로 혹은 valid path csv 경로가 들어옴\n",
        "# => return data_path_list\n",
        "def load_path_list(path) :\n",
        "    print(f\"[INFO] load data path list from {path}\")\n",
        "\n",
        "    data_path_list = []\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            data_path_list.append((line[0], line[1]))\n",
        "\n",
        "    return data_path_list\n",
        "\n",
        "# id, char 가 적혀있는 csv 파일 읽어오기\n",
        "# => return id2char (-> type : dictionary)\n",
        "def load_id2char(path) :\n",
        "    print(f\"[INFO] load id2char from {path}\")\n",
        "    \n",
        "    id2char = {}\n",
        "\n",
        "    with open(path, 'r', encoding='ms949') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            id2char[line[0]] = line[1]\n",
        "    \n",
        "    return id2char"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGVXqcTD5y3"
      },
      "source": [
        "# TODO\n",
        "# audio -> feature vector\n",
        "# => return feature vector\n",
        "def audio_to_featureVector(audio_path) :\n",
        "    pass\n",
        "\n",
        "# TODO\n",
        "# 0. batch size만큼의 data path list 받아서 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "# 1. batch내의 max length로 padding\n",
        "# => return inputs, input_lengths, targets, target_lengths\n",
        "def load_data(batch_data_list) :\n",
        "    pass"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkoAlbfPNL8",
        "outputId": "db92caac-c017-4334-9f3d-3502427e15f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "# !pip install python-Levenshtein\n",
        "import Levenshtein as Lev\n",
        "\n",
        "total_dist = 0.0\n",
        "total_length = 0.0\n",
        "EOS_ID = 0\n",
        "id2char = load_id2char() # TODO -> parameter : id, char 가 적혀있는 csv 파일 읽어오기\n",
        "\n",
        "def label_to_string(labels) : \n",
        "    sentences = str()\n",
        "    for label in labels :\n",
        "        if label.item() == EOS_ID :\n",
        "            break\n",
        "        sentence += id2char[label.item()]\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "def charErrorRate(targets, hypothesises) :\n",
        "    for target, hypothesis in zip(targets, hypothesises) :\n",
        "        s1 = label_to_string(target)\n",
        "        s2 = label_to_string(hypothesis)\n",
        "\n",
        "        # space 제거\n",
        "        s1 = s1.replace(' ', '')\n",
        "        s2 = s2.replace(' ', '')\n",
        "\n",
        "        # TODO : check -> sentence에 '_'가 있는지 없는지 확인, 있다면 '_'도 지우기\n",
        "\n",
        "        dist = Lev.distance(s2, s1)\n",
        "        length = len(s1)\n",
        "\n",
        "        total_dist += dist\n",
        "        total_length += length\n",
        "\n",
        "    return total_dist / total_length"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144789 sha256=7bbc70582b9f4a114180dfe6f4b859304325d194855d6e198b93824ac143d875\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7787fe86fca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mEOS_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mid2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_id2char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO -> parameter : id, char 가 적혀있는 csv 파일 읽어오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlabel_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_id2char() missing 1 required positional argument: 'path'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsApHJTBU0l"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2pDfJGBWl6"
      },
      "source": [
        "CHECKPOINT_SAVE_PATH = './backup'\n",
        "\n",
        "def model_save(model, optimizer) :\n",
        "    date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())\n",
        "    \n",
        "    trainer_states = {\n",
        "        'optimizer' : optimizer,\n",
        "        'model' : model\n",
        "    }\n",
        "\n",
        "    torch.save(trainer_states, os.path.join(CHECKPOINT_SAVE_PATH, date_time+\".pt\"))\n",
        "\n",
        "def model_load(model_name) :\n",
        "    trainer_states = torch.load(os.path.join(CHECKPOINT_SAVE_PATH, model_name))\n",
        "\n",
        "    return trainer_states[model], trainer_states[optimizer]\n",
        "\n",
        "# TODO : checkpoint가 저장되는 형식보고 작성하기\n",
        "def model_load_latest(checkpoint_dir) :\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtjdwSnG8Ik"
      },
      "source": [
        "## Train & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XlgaQuTJarq"
      },
      "source": [
        "def train_step(model, epoch, train_dataset, loss_func, optimizer, device='cuda') :\n",
        "    model.train() # model을 train mode로 변경\n",
        "    \n",
        "    inputs, input_lengths, targets, target_lengths = train_dataset\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.flatten_parameters()\n",
        "    else :\n",
        "        model.flatten_parameters()\n",
        "\n",
        "    result = model(inputs, input_lengths, targets)\n",
        "    result = torch.stack(result, dim=1).to(device) # list를 dim=1 방향으로 concatenate => return Tensor\n",
        "    hypothesises = result.max(-1)[1] # 확률이 제일 높은 index를 뽑아옴.\n",
        "\n",
        "    # loss 계산\n",
        "    targets = targets[:, 1:] # 0번째 column을 뺌 (모두 1임.)\n",
        "    loss = loss_func(result.contiguous().view(-1, result.size(-1)), targets.contiguous().view(-1))\n",
        "    step_loss = loss.item() # loss.item()은 loss의 스칼라 값.\n",
        "\n",
        "    # 정확도 계산\n",
        "    cer = charErrorRate(targets, hypothesises)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
        "\n",
        "    # torch.cuda.empty_cache() # TODO : check that this is necessary\n",
        "\n",
        "    return step_loss, cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEaSmddxL_h5"
      },
      "source": [
        "def validate(model, valid_dataset, device='cuda') :\n",
        "    print(\"[INFO] validate start\")\n",
        "    cer = 1.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad() :\n",
        "        inputs, input_lengths, targets, target_lengths = valid_dataset\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets[:, 1:].to(device)\n",
        "        model = model.to(device)\n",
        "\n",
        "        model.module.flatten_parameters()\n",
        "        result = model(inputs, input_lengths)\n",
        "        result = torch.stack(result, dim=1).to(device)\n",
        "\n",
        "        hypothesises = result.max(-1)[1]\n",
        "        cer = charErrorRate(targets, hypothesises)\n",
        "    \n",
        "    return cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5jdW4rFwQB"
      },
      "source": [
        "def train(model, train_path_list, valid_path_list, batch_size, num_epochs, lr, weight_decay) :    \n",
        "    print(\"[INFO] train start\")\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "    optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(num_epochs) :\n",
        "        # TODO\n",
        "        # -> batch size만큼 train_path_list에서 가져오기\n",
        "        # 1. batch size만큼 data load하기 \n",
        "        # -> csv 파일 내에 있는 data path를 통해 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        epoch_cer = 1.0\n",
        "        train_step_num = 0\n",
        "        batch = batch_size + 1\n",
        "\n",
        "        start = time.time()\n",
        "        while batch < len(train_path_list) :\n",
        "            audio_paths = train_path_list[:batch]\n",
        "\n",
        "            # train\n",
        "            batch_loss, batch_cer = train_step(model, epoch, load_data(audio_paths), loss_func, optimizer)\n",
        "\n",
        "            batch += batch_size + 1\n",
        "            epoch_loss += batch_loss\n",
        "            epoch_cer = batch_cer\n",
        "            train_step_num += 1\n",
        "\n",
        "        # valid\n",
        "        valid_paths = valid_path_list[:batch]\n",
        "        valid_cer = validate(model, load_data(valid_paths))\n",
        "\n",
        "        print(\"Epoch {} Loss {:.4f} \\t train_cer {:.4f} valid_cer {:.4f}\".format(epoch+1, epoch_loss/train_step_num, epoch_cer, valid_cer))\n",
        "        print(\"Time taken for 1 epoch : {} secs\\n\".format(time.time() - start))\n",
        "\n",
        "        # checkpoint 저장\n",
        "        if (epoch+1) % 5 == 0 :\n",
        "            model_save(model, optimizer)\n",
        "            print(\"[INFO] Lastest checkpoint restored!\")\n",
        "\n",
        "    \n",
        "    model_save(model, optimizer)\n",
        "    print(\"[INFO] Last checkpoint restored!\")\n",
        "    print(\"[INFO] Train Complete!! 🎶\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uceWMB3N7i9t"
      },
      "source": [
        "# tmp_input = torch.rand((BATCH_SIZE,951,N_MELS), dtype=torch.float64).uniform_(0,200)\n",
        "# tmp_input_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "# tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
        "# tmp_target_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "\n",
        "# tmp_input = tmp_input.float()\n",
        "# tmp_target = tmp_target.long()\n",
        "\n",
        "# train_dataset = (tmp_input, tmp_input_length, tmp_target, tmp_target_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIsWyc8HvXO1"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMNSrRyD73h"
      },
      "source": [
        "# hyper-parameter\n",
        "N_MELS = 80\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT_P = 0.15\n",
        "MAX_LEN = 150\n",
        "NUM_HEADS = 4\n",
        "ENC_NUM_LAYERS = 3\n",
        "DEC_NUM_LAYERS = 2\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = len(id2char) # dataset으로 label의 개수 넣어주기\n",
        "LEARNING_RATE = 1e-06\n",
        "WEIGHT_DECAY = 1e-05\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79"
      },
      "source": [
        "# load data\n",
        "train_path_list, valid_path_list = load_path_list() # TODO -> parameter : dataset_path\n",
        "\n",
        "model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=12, \n",
        "                                  num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "\n",
        "print(\"[INFO] model 초기화 성공\")\n",
        "\n",
        "train(model, train_path_list, valid_path_list, BATCH_SIZE, 1, LEARNING_RATE, WEIGHT_DECAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuFO-Q-zqRz"
      },
      "source": [
        "# 음성 파일 하나 받아서 결과 보여주기\n",
        "# parameter : model path, audio_path\n",
        "# => print(결과)\n",
        "def test(model_path, audio_path, device='cuda') :\n",
        "    # load audio => feature vector\n",
        "    feature_vector = audio_to_featureVector(audio_path)\n",
        "    input_length = torch.IntTensor([len(feature_vector)])\n",
        "\n",
        "    # load model\n",
        "    model = torch.load(model_path).to(device)\n",
        "\n",
        "    # TODO : check that this is necessary\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.decoder.device = device\n",
        "        model.module.encoder.device = device\n",
        "\n",
        "    else:\n",
        "        model.encoder.device = device\n",
        "        model.decoder.device = device\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # validate처럼 넣어주고\n",
        "    result = model(inputs=feature_vector, input_lengths=input_length)\n",
        "    result = torch.stack(result, dim=1).to(device)\n",
        "    pred = result.max(-1)[1]\n",
        "\n",
        "    # 나온 output을 string으로 바꿔준 후 \n",
        "    sentence = label_to_string(pred.cpu().detach().numpy())\n",
        "    # print함\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILxBdZEmedsl"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZdF67_gkgN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}