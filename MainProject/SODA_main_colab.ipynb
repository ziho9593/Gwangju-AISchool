{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kGzNOhkolhMq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt7cVIQlwKPi"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GbRK4o0qwPbf"
   },
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "    supported_rnns = {\n",
    "        'lstm': nn.LSTM,\n",
    "        'gru': nn.GRU,\n",
    "        'rnn': nn.RNN\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,                       # size of input\n",
    "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
    "            num_layers: int = 1,                   # number of recurrent layers\n",
    "            rnn_type: str = 'lstm',                # number of RNN layers\n",
    "            dropout_p: float = 0.3,                # dropout probability\n",
    "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
    "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
    "    ) -> None:\n",
    "        super(BaseRNN, self).__init__()\n",
    "        rnn_cell = self.supported_rnns[rnn_type]\n",
    "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pDFmoxfEwNjV"
   },
   "outputs": [],
   "source": [
    "class CNNExtractor(nn.Module):\n",
    "    supported_activations = {\n",
    "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
    "        'relu': nn.ReLU(inplace=True),\n",
    "        'elu': nn.ELU(inplace=True),\n",
    "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
    "        'gelu': nn.GELU()\n",
    "    }\n",
    "\n",
    "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
    "        super(CNNExtractor, self).__init__()\n",
    "        self.activation = CNNExtractor.supported_activations[activation]\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gqYrXsg3wYLz"
   },
   "outputs": [],
   "source": [
    "class VGGExtractor(CNNExtractor):\n",
    "    def __init__(self, activation: str, mask_conv: bool) :\n",
    "        super(VGGExtractor, self).__init__(activation)\n",
    "        self.mask_conv = mask_conv\n",
    "        self.conv = nn.Sequential(\n",
    "            # block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            self.activation,\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            # block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            self.activation,\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            # block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            self.activation,\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            self.activation,\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            # block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            # block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
    "        conv_feat = self.conv(inputs)\n",
    "        output = conv_feat\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NypewFuYwMEq"
   },
   "outputs": [],
   "source": [
    "class Listener(BaseRNN):\n",
    "  def __init__(\n",
    "            self,\n",
    "            input_size: int,                       # size of input\n",
    "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
    "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
    "            dropout_p: float = 0.3,                # dropout probability\n",
    "            num_layers: int = 3,                   # number of RNN layers\n",
    "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
    "            rnn_type: str = 'lstm',                # type of RNN cell\n",
    "            extractor: str = 'vgg',                # type of CNN extractor\n",
    "            activation: str = 'hardtanh',          # type of activation function\n",
    "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
    "    ) -> None:\n",
    "        self.mask_conv = mask_conv\n",
    "        self.extractor = extractor.lower()\n",
    "        self.device = device\n",
    "\n",
    "        if self.extractor == 'vgg':\n",
    "            # input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
    "            input_size = 1024\n",
    "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
    "            self.conv = VGGExtractor(activation, mask_conv)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
    "\n",
    "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
    "    conv_feat = conv_feat.transpose(1, 2)\n",
    "\n",
    "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
    "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
    "\n",
    "    if self.training:\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "    output, hidden = self.rnn(conv_feat)\n",
    "\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfzxPWgIZEQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqHgOCkJec9C"
   },
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tUweJ9vfCBRZ"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask) :\n",
    "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
    "\n",
    "    if mask is not None :\n",
    "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
    "\n",
    "    attention_weights = F.softmax(scaled_attention_logits, -1)\n",
    "    output = torch.bmm(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0Vss_cRBJfcn"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, d_model=512, num_heads=8) :\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None) :        \n",
    "        batch_size = v.size(0)\n",
    "\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
    "\n",
    "        # split heads\n",
    "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
    "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
    "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
    "\n",
    "        if mask is not None :\n",
    "            mask = mask.repeat(self.num_heads, 1, 1)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
    "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.linear(scaled_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lSljKJ8nQMLE"
   },
   "outputs": [],
   "source": [
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "# out, attn = temp_mha(y, y, y, mask=None)\n",
    "\n",
    "# display(out.shape, attn.shape)\n",
    "# display(y)\n",
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjrOLCm4ehUP"
   },
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xlqlQXEGQ_kW"
   },
   "outputs": [],
   "source": [
    "class DecoderStep(nn.Module) :\n",
    "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
    "        super(DecoderStep, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(num_classes, d_model)\n",
    "        self.input_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.uniDirLSTM = nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.linear2 = nn.Linear(d_model, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, input_var, hidden, enc_output) :\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
    "\n",
    "        embedded = self.embedding(input_var).to(self.device)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "\n",
    "        if self.training :\n",
    "            self.uniDirLSTM.flatten_parameters()\n",
    "\n",
    "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
    "        \n",
    "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
    "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        out_proj = self.linear1(out2)\n",
    "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
    "\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
    "\n",
    "        return output, hidden, attn_weights_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9nEVQ3IdEjkn"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        # self.num_layers = num_layers\n",
    "\n",
    "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
    "\n",
    "    def forward(self, inputs, enc_outputs) :\n",
    "        assert enc_outputs is not None\n",
    "        \n",
    "        hidden = None\n",
    "        result = list()\n",
    "\n",
    "        batch_size = enc_outputs.size(0)\n",
    "\n",
    "        # validate\n",
    "        if inputs is None :\n",
    "            inputs = torch.LongTensor([1] * batch_size).view(batch_size, 1).to(self.device) # [sos_id] * batch_size\n",
    "            max_length = self.max_length\n",
    "        else :\n",
    "            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
    "\n",
    "        input_var = inputs[:, 0].unsqueeze(1)\n",
    "        for di in range(max_length) :\n",
    "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
    "            result.append(step_output)\n",
    "            input_var = result[-1].topk(1)[1]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM11tHrMpzIR"
   },
   "source": [
    "## LAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "19VWgRi5H-Vp"
   },
   "outputs": [],
   "source": [
    "class LAS(nn.Module) :\n",
    "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
    "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
    "        super(LAS, self).__init__()\n",
    "\n",
    "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
    "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
    "\n",
    "    def forward(self, inputs, input_lengths, targets=None):\n",
    "        output, hidden = self.encoder(inputs, input_lengths)\n",
    "\n",
    "        result = self.decoder(targets, output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def flatten_parameters(self) :\n",
    "        self.encoder.rnn.flatten_parameters()\n",
    "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbf3XKq4D4bO"
   },
   "source": [
    "## data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CbYHyk6D1LN9"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# (train & valid) dataset path가 들어있는 csv 파일 읽어오기\n",
    "# parameter로 train path csv 경로 혹은 valid path csv 경로가 들어옴\n",
    "# => return data_path_list\n",
    "def load_path_list(train_path, valid_path) :\n",
    "    train_path_list, valid_path_list = [], []\n",
    "\n",
    "    print(f\"[INFO] load train path list from {train_path}\")\n",
    "    with open(train_path, 'r') as f:\n",
    "        r = csv.reader(f)\n",
    "        next(r)\n",
    "        for line in r:\n",
    "            train_path_list.append((line[0], line[1]))\n",
    "\n",
    "    print(f\"[INFO] load train path list from {valid_path}\")\n",
    "    with open(valid_path, 'r') as f:\n",
    "        r = csv.reader(f)\n",
    "        next(r)\n",
    "        for line in r:\n",
    "            valid_path_list.append((line[0], line[1]))\n",
    "\n",
    "    return train_path_list, valid_path_list\n",
    "\n",
    "# id, char 가 적혀있는 csv 파일 읽어오기\n",
    "# => return id2char (-> type : dictionary)\n",
    "def load_id2char(path) :\n",
    "    print(f\"[INFO] load id2char from {path}\")\n",
    "    \n",
    "    id2char = {}\n",
    "\n",
    "    with open(path, 'r', encoding='ms949') as f:\n",
    "        r = csv.reader(f)\n",
    "        next(r)\n",
    "        for line in r:\n",
    "            id2char[line[0]] = line[1]\n",
    "    \n",
    "    return id2char\n",
    "\n",
    "# train_list를 shuffle해서\n",
    "# csv 파일로 저장\n",
    "def train_list_shuffle(train_path_list) :\n",
    "    if len(train_path_list) > 50000 :\n",
    "        train_path_list = train_path_list[:50000]\n",
    "    \n",
    "    random.shuffle(train_path_list)\n",
    "        \n",
    "    with open('train_list_02.csv', 'w', newline='', encoding='ms949') as f :\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['audio','label'])\n",
    "        for path in train_path_list :\n",
    "            writer.writerow([path[0], path[1]])\n",
    "        \n",
    "    print(\"[INFO] train list shuffle done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ygnl0Uk8_p1"
   },
   "source": [
    "#### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmEcNMPA9Crt",
    "outputId": "66423176-99e2-48eb-dcbf-3d9fa15c9f6c"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.13.2\n",
    "# # !pip3 install SpecAugment\n",
    "# !pip install SpecAugment==1.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DBbM9v3pHw3",
    "outputId": "4fec13ee-8e6a-423b-a0b2-207e2c584377",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ansdu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from specAugment import spec_augment_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oDArguH69iEn"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from scipy.fftpack import dct\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lSd3pezNo63j"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from specAugment import spec_augment_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lYULYIur8rZ_"
   },
   "outputs": [],
   "source": [
    "# audio -> feature vector\n",
    "# => return feature vector\n",
    "def audio_to_featureVector(audio_path, n_mels, noise_injection=False) :\n",
    "    signal = np.memmap(os.path.join(os.getcwd(), 'original', audio_path), dtype='h', mode='r').astype('float32') # load audio\n",
    "    data = signal / 32767   # normalize audio\n",
    "\n",
    "    # noise injection\n",
    "    if noise_injection :\n",
    "        wn = np.random.randn(len(data))\n",
    "        data_wn = data + 0.005*wn\n",
    "    else:\n",
    "        data_wn = data\n",
    "\n",
    "    sr = 16000\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=data_wn, sr=sr, n_mels=256, hop_length=128, fmax=8000) # data to melspectrogram\n",
    "\n",
    "    mfcc = dct(librosa.core.power_to_db(mel_spectrogram), type=2, axis=1, norm='ortho')[:n_mels] # mel spectrogram to mfcc\n",
    "\n",
    "    # warped_masked_spectrogram = spec_augment_tensorflow.spec_augment(mel_spectrogram=mel_spectrogram, time_warping_para=30) # melspectrogram spec augmentation\n",
    "\n",
    "    # mfcc = dct(librosa.core.power_to_db(warped_masked_spectrogram), type=2, axis=1, norm='ortho')[:n_mels] # mel spectrogram to mfcc\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# 0. batch size만큼의 data path list 받아서 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
    "# 1. batch내의 max length로 padding\n",
    "# => return inputs, input_lengths, targets, target_lengths\n",
    "def load_data(batch_data_path_list, n_mels):\n",
    "    mfcc_list, input_lengths, target_list, target_lengths = [], [], [], []\n",
    "\n",
    "    max_mfcc_shape = 0\n",
    "    max_target_shape = 0\n",
    "    for i in batch_data_path_list :\n",
    "        # audio to feature vector\n",
    "        mfcc = audio_to_featureVector(i[0], n_mels, False) \n",
    "        mfcc_list.append(mfcc)\n",
    "        input_lengths.append(mfcc.shape[1])\n",
    "        # for padding\n",
    "        if max_mfcc_shape < mfcc.shape[1] :\n",
    "            max_mfcc_shape = mfcc.shape[1]\n",
    "\n",
    "        # target list\n",
    "        with open(os.path.join(os.getcwd(), 'original', i[1]), 'r') as f :\n",
    "            label = f.readline()\n",
    "            label = list(map(int, label.split()))\n",
    "            target_list.append(label)\n",
    "            target_lengths.append(len(label))\n",
    "            # for padding\n",
    "            if max_target_shape < len(label) :\n",
    "                max_target_shape = len(label)\n",
    "\n",
    "    # train padding\n",
    "    inputs = []\n",
    "    for mfcc in mfcc_list :\n",
    "        padding_shape = np.zeros((n_mels, max_mfcc_shape))\n",
    "        padding_shape[:mfcc.shape[0],:mfcc.shape[1]] = mfcc\n",
    "        inputs.append(padding_shape)\n",
    "\n",
    "    # target padding\n",
    "    padding_targets = []\n",
    "    for target in target_list :\n",
    "        target_padding_shape = np.zeros(max_target_shape+2)\n",
    "        target.insert(0,1) # 맨 앞에 sos_id 추가\n",
    "        target.append(2) # 맨 뒤에 eos_id 추가\n",
    "        target_padding_shape[:len(target)] = target\n",
    "        padding_targets.append(target_padding_shape)\n",
    "\n",
    "    inputs = torch.FloatTensor(inputs).permute(0,2,1)\n",
    "    input_lengths = torch.IntTensor(input_lengths)\n",
    "    padding_targets = torch.LongTensor(padding_targets)\n",
    "\n",
    "    return inputs, input_lengths, padding_targets, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwv1JKKfRuRv"
   },
   "source": [
    "### 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nifPs1l3pUiR",
    "outputId": "0c145247-b973-457a-9427-038f2d49e277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansdu\\Desktop\\main\\data\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\ansdu\\Desktop\\main\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-Levenshtein-wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkoAlbfPNL8",
    "outputId": "cac11c2c-7542-40d7-8e91-b18055151ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] load id2char from aihub_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-Levenshtein\n",
    "import Levenshtein as Lev\n",
    "\n",
    "total_dist = 0.0\n",
    "total_length = 0.0\n",
    "EOS_ID = 2\n",
    "id2char = load_id2char(\"aihub_labels.csv\")\n",
    "\n",
    "def label_to_string(labels) : \n",
    "    if len(labels.shape) == 1:\n",
    "        sentence = str()\n",
    "        for label in labels:\n",
    "            if label.item() == EOS_ID :\n",
    "                break\n",
    "            if id2char[str(label.item())] == '^' :\n",
    "                sentence += \"(웃음)\"\n",
    "            elif str(label.item()) == '3' :\n",
    "                sentence += \" \"\n",
    "            else :\n",
    "                sentence += id2char[str(label.item())]\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    sentences = list()\n",
    "    for batch in labels:\n",
    "        sentence = str()\n",
    "        for label in batch:\n",
    "            if label.item() == EOS_ID :\n",
    "                break\n",
    "            if id2char[str(label.item())] == '^' :\n",
    "                sentence += \"(웃음)\"\n",
    "            elif str(label.item()) == '3' :\n",
    "                sentence += \" \"\n",
    "            else :\n",
    "                sentence += id2char[str(label.item())]\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def charErrorRate(targets, hypothesises) :\n",
    "    total_dist = 0\n",
    "    total_length = 0\n",
    "\n",
    "    for target, hypothesis in zip(targets, hypothesises) :\n",
    "        s1 = label_to_string(target)\n",
    "        s2 = label_to_string(hypothesis)\n",
    "\n",
    "        # space 제거\n",
    "        s1 = s1.replace(' ', '')\n",
    "        s2 = s2.replace(' ', '')\n",
    "\n",
    "        # '_' 제거\n",
    "        s1 = s1.replace('_', '')\n",
    "        s2 = s2.replace('_', '')\n",
    "\n",
    "        dist = Lev.distance(s2, s1)\n",
    "        length = len(s1)\n",
    "\n",
    "        total_dist += dist\n",
    "        total_length += length\n",
    "\n",
    "    return 100 * (1 - total_dist/total_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfsApHJTBU0l"
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cQ2pDfJGBWl6"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_SAVE_PATH = \"../backup\"\n",
    "had_header = False\n",
    "\n",
    "def model_save(model, optimizer, loss_cer, train_index, valid_index) :\n",
    "    global had_header\n",
    "    date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime(time.time()))\n",
    "\n",
    "    trainer_states = {\n",
    "        'train_index' : train_index,\n",
    "        'valid_index' : valid_index,\n",
    "        'epoch' : loss_cer['epoch'],\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        'model' : model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(trainer_states, os.path.join(CHECKPOINT_SAVE_PATH, \"model_\"+date_time+\".pt\")) # model save\n",
    "    torch.save(model, os.path.join(CHECKPOINT_SAVE_PATH, \"total_model_\"+date_time+\".pt\")) # model save\n",
    "    print(\"model name : total_model_\"+date_time+\".pt\")\n",
    "\n",
    "    # loss & cer save\n",
    "    loss_cer['date_time'] = date_time\n",
    "    with open(os.path.join(CHECKPOINT_SAVE_PATH, 'loss_cer.csv'), 'a', newline='', encoding='ms949') as f :\n",
    "        w = csv.DictWriter(f, loss_cer.keys())\n",
    "\n",
    "        if not had_header :\n",
    "            w.writeheader()\n",
    "            had_header = True\n",
    "\n",
    "        w.writerow(loss_cer)\n",
    "\n",
    "    return os.path.join(CHECKPOINT_SAVE_PATH, \"model_\"+date_time+\".pt\")\n",
    "\n",
    "def model_load(model_name) :\n",
    "    checkpoint = torch.load(os.path.join(CHECKPOINT_SAVE_PATH, model_name))\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFtjdwSnG8Ik"
   },
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2XlgaQuTJarq"
   },
   "outputs": [],
   "source": [
    "def train_step(model, epoch, train_dataset, loss_func, optimizer, device='cuda') :\n",
    "    model.train() # model을 train mode로 변경\n",
    "    \n",
    "    inputs, input_lengths, targets, target_lengths = train_dataset\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.flatten_parameters()\n",
    "    else :\n",
    "        model.flatten_parameters()\n",
    "\n",
    "    result = model(inputs, input_lengths, targets)\n",
    "    result = torch.stack(result, dim=1).to(device) # list를 dim=1 방향으로 concatenate => return Tensor\n",
    "    hypothesises = result.max(-1)[1] # 확률이 제일 높은 index를 뽑아옴.\n",
    "\n",
    "    # loss 계산\n",
    "    targets = targets[:, 1:] # 0번째 column을 뺌 (모두 1임.)\n",
    "    loss = loss_func(result.contiguous().view(-1, result.size(-1)), targets.contiguous().view(-1))\n",
    "    step_loss = loss.item() # loss.item()은 loss의 스칼라 값.\n",
    "\n",
    "    # 정확도 계산\n",
    "    cer = charErrorRate(targets, hypothesises)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return step_loss, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zEaSmddxL_h5"
   },
   "outputs": [],
   "source": [
    "def validate(model, valid_dataset, device='cuda') :\n",
    "    print(\"[INFO] validate start\")\n",
    "    cer = 1.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad() :\n",
    "        inputs, input_lengths, targets, target_lengths = valid_dataset\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets[:, 1:].to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.flatten_parameters()\n",
    "        else :\n",
    "            model.flatten_parameters()\n",
    "            \n",
    "        result = model(inputs, input_lengths)\n",
    "        result = torch.stack(result, dim=1).to(device)\n",
    "\n",
    "        hypothesises = result.max(-1)[1]\n",
    "        cer = charErrorRate(targets, hypothesises)\n",
    "    \n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mz5jdW4rFwQB"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_path_list, valid_path_list, batch_size, num_epochs, n_mels, start_epoch, train_index, valid_index) :\n",
    "    count = 0\n",
    "    model_name = \"\"\n",
    "    print(\"[INFO] train start\")\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    valid_batch = valid_index\n",
    "    for epoch in range(start_epoch, num_epochs) :\n",
    "        # -> batch size만큼 train_path_list에서 가져오기\n",
    "        # 1. batch size만큼 data load하기 \n",
    "        # -> csv 파일 내에 있는 data path를 통해 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_cer = 1.0\n",
    "        train_step_num = 0\n",
    "        batch = train_index\n",
    "        if batch > 50000 :\n",
    "            batch = batch_size\n",
    "\n",
    "        start = time.time()\n",
    "        while batch <= 50000 :\n",
    "            audio_paths = train_path_list[batch-batch_size : batch]\n",
    "\n",
    "            # train\n",
    "            batch_loss, batch_cer = train_step(model, epoch, load_data(audio_paths, n_mels), loss_func, optimizer)\n",
    "            print(\"Batch {}\".format(batch))\n",
    "\n",
    "            batch += batch_size\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_cer = batch_cer\n",
    "            train_step_num += 1\n",
    "\n",
    "            if (batch-batch_size) % 1000 == 0 :\n",
    "                loss_cer = {\n",
    "                    'epoch' : epoch+1,\n",
    "                    'loss' : batch_loss,\n",
    "                    'tmp_cer' : batch_cer,\n",
    "                    'valid_cer' : 0.0\n",
    "                }\n",
    "\n",
    "                model_name = model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
    "                print(\"batch loss : {:.4f} \\t tmp cer : {:.4f}\".format(batch_loss, batch_cer))\n",
    "                print(\"[INFO] Lastest checkpoint restored at batch {}!\".format(batch-batch_size))\n",
    "                \n",
    "        train_index = batch\n",
    "        # valid\n",
    "        if valid_batch <= len(valid_path_list) :\n",
    "            valid_paths = valid_path_list[valid_batch-batch_size : valid_batch]\n",
    "            valid_cer = validate(model, load_data(valid_paths, n_mels))\n",
    "            valid_batch += batch_size\n",
    "\n",
    "        print(\"Epoch {} Loss {:.4f} \\t train_cer {:.4f}% valid_cer {:.4f}%\".format(epoch+1, epoch_loss/train_step_num, epoch_cer, valid_cer))\n",
    "        print(\"Time taken for 1 epoch : {} secs\\n\".format(time.time() - start))\n",
    "\n",
    "        loss_cer = {\n",
    "            'epoch' : epoch+1,\n",
    "            'loss' : epoch_loss,\n",
    "            'tmp_cer' : epoch_cer,\n",
    "            'valid_cer' : valid_cer\n",
    "        }\n",
    "\n",
    "        # 매 epoch마다 checkpoint 저장\n",
    "        model_name = model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
    "        print(\"[INFO] Lastest checkpoint restored!\")\n",
    "        \n",
    "        # train list shuffle\n",
    "        train_list_shuffle(train_path_list)\n",
    "\n",
    "    model_name = model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
    "    print(\"[INFO] Last checkpoint restored!\")\n",
    "    print(\"[INFO] Train Complete!! 🎶\")\n",
    "\n",
    "    return model_name, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uceWMB3N7i9t"
   },
   "outputs": [],
   "source": [
    "# tmp_input = torch.rand((BATCH_SIZE,951,N_MELS), dtype=torch.float64).uniform_(0,200)\n",
    "# tmp_input_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
    "# tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
    "# tmp_target_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
    "\n",
    "# tmp_input = tmp_input.float()\n",
    "# tmp_target = tmp_target.long()\n",
    "\n",
    "# train_dataset = (tmp_input, tmp_input_length, tmp_target, tmp_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIsWyc8HvXO1"
   },
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wOMNSrRyD73h"
   },
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "N_MELS = 80\n",
    "HIDDEN_DIM = 8\n",
    "DROPOUT_P = 0.15\n",
    "MAX_LEN = 150\n",
    "NUM_HEADS = 4\n",
    "ENC_NUM_LAYERS = 3\n",
    "DEC_NUM_LAYERS = 2\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "NUM_CLASSES = len(id2char) # dataset으로 label의 개수 넣어주기\n",
    "LEARNING_RATE = 1e-06\n",
    "WEIGHT_DECAY = 1e-05\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYkiah1V2rWT"
   },
   "source": [
    "#### 처음 학습시킬 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EwnGe2zmHU79",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_path_list, valid_path_list = load_path_list(\"train_list_02.csv\", \"test_list_02.csv\")\n",
    "\n",
    "# model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
    "#                                   num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
    "# optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "model = LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
    "                                num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "print(\"[INFO] model 초기화 성공\")\n",
    "\n",
    "model_name, check = train(model, optimizer, train_path_list, valid_path_list, BATCH_SIZE, NUM_EPOCHS, N_MELS, 0, BATCH_SIZE, BATCH_SIZE)\n",
    "\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEA15Vw52uOL"
   },
   "source": [
    "#### checkpoint 가지고 학습시킬 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VqBjNg5B2wrO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_path_list, valid_path_list = load_path_list(\"train_list_02.csv\", \"test_list_02.csv\")\n",
    "\n",
    "model_name = \"../backup/model_2020_11_22_20_31_12.pt\"\n",
    "\n",
    "print(\"[INFO] checkpoint load\")\n",
    "model = LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
    "                                num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# model load\n",
    "checkpoint = model_load(model_name)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "train_index, valid_index, start_epoch = checkpoint['train_index'], checkpoint['valid_index'], checkpoint['epoch']\n",
    "print(\"[INFO] model load 성공\")\n",
    "\n",
    "model_name, check = train(model, optimizer, train_path_list, valid_path_list, BATCH_SIZE, NUM_EPOCHS, N_MELS, start_epoch-1, train_index, valid_index)\n",
    "print(\"✨\")\n",
    "\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRYXjhUtNY4B"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4DuFO-Q-zqRz"
   },
   "outputs": [],
   "source": [
    "# 음성 파일 하나 받아서 결과 보여주기\n",
    "# parameter : model path, audio_path\n",
    "# => print(결과)\n",
    "def test(model_path, audio_path, n_mels=80, device='cuda') :\n",
    "    # load audio => feature vector\n",
    "    feature_vector = audio_to_featureVector(audio_path, n_mels)\n",
    "    feature_vector = torch.FloatTensor(feature_vector).transpose(0, 1).to(device)\n",
    "\n",
    "    input = feature_vector.unsqueeze(0)\n",
    "    input_length = torch.IntTensor([len(feature_vector)]).to(device)\n",
    "\n",
    "    # load model\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.decoder.device = device\n",
    "        model.module.encoder.device = device\n",
    "    else:\n",
    "        model.encoder.device = device\n",
    "        model.decoder.device = device\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # validate처럼 넣어주고\n",
    "    result = model(inputs=input, input_lengths=input_length)\n",
    "    result = torch.stack(result, dim=1).to(device)\n",
    "    pred = result.max(-1)[1]\n",
    "\n",
    "    # 나온 output을 string으로 바꿔준 후\n",
    "    sentence = label_to_string(pred.cpu().detach().numpy())\n",
    "    \n",
    "    # print함\n",
    "    with open(audio_path.replace(\".pcm\", \".txt\"), 'r', encoding='ms949') as f :\n",
    "        print(\"original : \", f.readline())\n",
    "    print(\"predict : \", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ILxBdZEmedsl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original :  b/ 작년에 배추값 엄청 올랐었잖아.\n",
      "\n",
      "predict :  ['퀭퀭근탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰벰벰역역말돔룟벛돔탔탔벰']\n"
     ]
    }
   ],
   "source": [
    "model_path = 'C:/Users/ansdu/Desktop/main/total_model_2020_11_22_21_26_44.pt'\n",
    "audio_path = 'C:/Users/ansdu/Desktop/main/data/original/KsponSpeech_02/KsponSpeech_0126/KsponSpeech_125005.pcm'\n",
    "\n",
    "test(model_path, audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSZdF67_gkgN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Qt7cVIQlwKPi",
    "nqHgOCkJec9C",
    "EjrOLCm4ehUP",
    "GM11tHrMpzIR",
    "iwv1JKKfRuRv"
   ],
   "machine_shape": "hm",
   "name": "SODA_main_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
