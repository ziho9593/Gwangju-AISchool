{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 광주 인공지능 사관학교\n",
    "- - -\n",
    "- 작성자 : 2반 한지호\n",
    "- 작성일 : 20.07.15 수\n",
    "- 3교시 딥러닝 시간에 진행한 '전복 고리 수 추정' 예제 클론 코딩 실습\n",
    "- 단층 퍼셉트론의 이해를 위해 예제 코드를 강사님과 함께 보며 구조와 그 이유를 파악하였다.\n",
    "- A, B Line 까지 오늘 진행했고, 다음 수업시간 진도에 맞춰 추가 작성할 예정\n",
    "- - -\n",
    "- 수정일 : 20.07.16 목\n",
    "- C Line ~ D.1 & E.1 까지 진행, 다음 수업시간에 이어 진행할 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flowchart](./flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 파이썬 모듈 불러들이기  \n",
    "- 이 예제는 단층 퍼셉트론의 기본 구조를 파악하기 위해 라이브러리 사용을 최소화한 예제다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 하이퍼 파라미터 정의  \n",
    "- 하이퍼 파라미터 : 고정값 (한 번의 실험 사이클 중간에 절대 변경 불가능한 값)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "\n",
    "LEARNING_RATE = 0.001 # 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 실험용 메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abalone_exec(epoch_count=10, mb_szie=10, report=1):\n",
    "    load_abalone_dataset() # 데이터를 불러들이는 함수\n",
    "    init_model() # 모델 초기화 함수\n",
    "    train_and_test(epoch_count, mb_szie, report) # 학습 및 테스트 수행 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 데이터 적재함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abalone_dataset():\n",
    "    with open('abalone.csv') as f:\n",
    "        r = csv.reader(f)\n",
    "        next(r, None) # next()로 첫 행을 None으로 처리 (원하는 데이터만 출력 = 불필요한 정보를 처리)  \n",
    "        rows = []\n",
    "        for row in r:\n",
    "            rows.append(row)\n",
    "    \n",
    "    global data, input_count, output_count # 전역변수 생성\n",
    "    # 데이터의 입출력 벡터 정보 저장. 이후 크기 지정에 활용. \n",
    "    input_count, output_count = 10, 1 #원 핫(3) + 속성(7) = 10, 레이블 = 1\n",
    "    data = np.zeros(len(rows), input_count+output_count) \n",
    "    \n",
    "    # 원-핫 벡터 처리\n",
    "    # I = 1,0,0 / M = 0,1,0 / F = 0,0,1\n",
    "    for n, row in enumerate(rows):\n",
    "        # data[n, 0~3] = 성별 정보 원 핫 벡터 처리 / 이후 데이터(속성)는 그 뒤에 그대로 복사\n",
    "        if row[0] == 'I': \n",
    "            data[n, 0] = 1 \n",
    "        if row[0] == 'M': \n",
    "            data[n, 1] = 1 \n",
    "        if row[0] == 'F': \n",
    "            data[n, 2] = 1 \n",
    "        data[n, 3:] = row[1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 파라미터 초기화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    global weight, bias, input_count, output_count # 전역변수 불러오기 및 생성\n",
    "    weight = np.random.normal(RND_MEAN, RND_STD, [input_count, output_count]) # 가중치 초기화 : 정규분포를 갖는 난수 생성\n",
    "    bias = np.zeros([output_count]) # 편향 0으로 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 학습 및 평가 함수 정의\n",
    "- 1 epoch = 1 batch (= N mini batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_count, mb_size, report):\n",
    "    step_count = arrange_data(mb_size) \n",
    "    test_x, test_y = get_test_data() \n",
    "    \n",
    "    for epoch in range(epoch_count): # epoch_count 만큼 epoch 반복 수행\n",
    "        losses, accs = [], [] # 한 차례의 epoch마다 손실과 정확도 저장\n",
    "        \n",
    "        for n in range(step_count): # 학습 데이터 크기에 비례하여 (80%) 미니배치 처리된 횟수 만큼 반복 수행\n",
    "            train_x, train_y = get_train_data(mb_size, n) # 미니배치 마다의 학습 데이터 분할\n",
    "            loss, acc = run_train(train_x, train_y) # 학습 수행 및 손실과 정확도 산출\n",
    "            # 미니배치 처리 이후 손실과 정확도를 누적하여 저장 (이후 이 값들을 평균내면 한차례의 'epoch' 처리) \n",
    "            losses.append(loss) \n",
    "            accs.append(acc) \n",
    "            \n",
    "        if report > 0 and (epoch+1) % report == 0: # 출력 주기 및 테스트 주기 설정\n",
    "            acc = run_test(test_X, test_y) # 테스트 데이터로 테스트 진행\n",
    "            print(f'Epoch {epoch+1}: loss={np.mean(losses):5.3f}, accuracy={np.mean(accs):5.3f}/{acc:5.3f}')\n",
    "            \n",
    "        final_acc = run_test(test_x, test_y) # 모든 반복이 종료되었을 때, 한 번 더 최종 결과 출력\n",
    "        print(f'\\nFinal Test: final accuracy = {final_acc:5.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1~3 학습 및 평가 데이터 획득 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(mb_size):\n",
    "    global data, shuffle_map, test_begin_idx\n",
    "    shuffle_map = np.arange(data.shape[0]) # 데이터의 순서값을 생성\n",
    "    np.random.shuffle(shuffle_map) # 데이터를 무작위로 섞어주는 과정 (중복 방지) \n",
    "    step_count = int(data.shape[0] * 0.8) // mb_size # 데이터의 80% 기준, 미니배치 사이즈에 의한 1 epoch 당 미니배치 출력 횟수 출력\n",
    "    test_begin_idx = step_count * mb_size # 학습 데이터와 테스트 데이터의 경계선 인덱스 생성\n",
    "    return step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(): # 테스트 데이터를 일괄 공급하는 함수\n",
    "    global data, shuffle_map, test_begin_idx, output_count\n",
    "    test_data = data[shuffle_map[test_begin_idx:]] # 테스트 데이터 생성 \n",
    "    return test_data[:, :-output_count], test_data[:, -output_count:] # 테스트 데이터의 종속 변수와 독립 변수를 분할하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(mb_size, nth): # mb_size : 미니배치 크기 / nth : 미니배치 실행 순서\n",
    "    global data, shuffle_map, test_begin_idx, output_count\n",
    "    if nth == 0: # 첫 epoch마다 한하여\n",
    "        np.random.shuffle(shuffle_map[:test_begin_idx]) # 처음부터 경계선까지 인덱스를 섞어준다.\n",
    "    train_data = data[shuffle_map[mb_size*nth : mb_size*(nth+1)]] # 섞인 인덱스로 ㅣ니배치 크기에 맞게 데이터 분할 및 train_data로 저장\n",
    "    return train_data[:, :-output_count], test_data[:, -output_count:] # 학습 데이터의 종속 변수와 독립 변수를 분할하여 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 학습 및 평가 데이터 획득 함수 정의\n",
    "1. 순전파 과정  \n",
    "  \\- 순전파의 뒷 단계 '후처리 과정' : forward_postproc() -> loss\n",
    "2. 역전파 과정  \n",
    "  \\- 손실에 대한 기울기 : G_loss  \n",
    "  \\- 순전파의 뒷 단계였던 후처리 과정에 대한 역전파 함수 backprop_postproc()가 먼저 호출  \n",
    "  \\- backprop_postproc() -> G_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(x, y):\n",
    "    # 1. 순전파 및 정확도 추출 과정\n",
    "    output, aux_nn = forward_neuralnet(x) # 신경망 연산 부분 / aux_nn : 입력 벡터 'x' (보조 정보) \n",
    "    loss, aux_pp = forward_postproc(output, y) # 신경망 후처리 과정 (손실 함수를 구하는 과정) / aux_pp : 편차 diff\n",
    "    accuracy = eval_accuracy(output, y) # 정확도를 구하는 과정\n",
    "    \n",
    "    # 2. 역전파 과정 : 항상 순전파의 역순으로 수행\n",
    "    G_loss = 1.0 # 순전파 때 출력이었던 성분의 '손실 함수의 기울기' \n",
    "    G_output = backprop_postproc(G_loss, aux_pp) # 손실 함수의 처리 과정인 '평균 제곱 오차'의 역순. 즉, 'MSE의 역전파 처리' \n",
    "    # 직접적인 학습이 이뤄지는 부분 (가중치와 편향이 학습률을 활용하여 실제 학습 과정 수행) \n",
    "    backprop_neuralnet(G_output, aux_nn) # 입력값에 따른 f(x)에 대한 편미분 과정을 구해주는 내부처리\n",
    "    \n",
    "    return loss, accuracy # 손실 값과 정확도를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.5 학습 및 평가 데이터 획득 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(x, y): \n",
    "    # 순전파 과정 수행, 테스트에서는 역전파를 수행하지 않으므로 보조 정보가 필요하지 않다.\n",
    "    output, _ = forward_neuralnet(x) # 따라서 두 번째 반환값인 '추가 정보 반환'은 필요 없으므로 '_' 처리\n",
    "    accuracy = eval_accuracy(output, y) # 최종 정확도 추출\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.1 & E.1 단층 퍼셉트론에 대한 순전파 및 역전파 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet(x):\n",
    "    global weight, bias\n",
    "    output = np.matmul(x, weight) + bias # 편향이 더해진 입력 벡터와 가중치 벡터에 대한 기본적인 신경망 연산식 / np.matmul() : 두 배열의 행렬곱 연산\n",
    "    return output, x # 여기서 x는 역전파에 필요한 보조 정보(aux_nn, aux_pp)로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_neuralnet(G_output, x): # 입력 값에 따른 f(x)에 대한 편미분 과정에서 각각 가중치(G_w)와 편향(G_b)의 손실 기울기 연산\n",
    "    global weight, bias \n",
    "     \n",
    "    # 가중치에 댈한 손실 기울기 계산\n",
    "    g_output_w = x.transpose() # 가중치의 손실 기울기를 구하기 위해 필요한 값에 대한 사전 작업 / .transpose() : 해당 편수에 대한 전치 행렬 수행\n",
    "    G_w = np.matmul(g_output_w, G_output) \n",
    "    \n",
    "    # 편향에 대한 손실 기울기 계산\n",
    "    G_b = np.sum(G_output, axis=0) \n",
    "    \n",
    "    # 가중치와 편향 갱신\n",
    "    weight -= LEARNING_RATE * G_w\n",
    "    bias -= LEARNING_RATE * G_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
